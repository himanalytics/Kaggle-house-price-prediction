{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import kaggle\n",
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Download dataset\n",
    "!kaggle competitions download -c house-prices-advanced-regression-techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "##Paths\n",
    "Data_Dir = Path(r\"C:\\Users\\chand\\Desktop\\ML Projects\\Kaggle- house-price\")\n",
    "train_path = Data_Dir/\"train.csv\"\n",
    "test_path = Data_Dir/\"test.csv\"\n",
    "desc_path = Data_Dir/\"data_description.txt\"\n",
    "\n",
    "##load\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "\n",
    "##Shapes\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"test shape:\", test.shape)\n",
    "\n",
    "#target tansform for metrix\n",
    "y= np.log1p(train[\"SalePrice\"].values)\n",
    "X=train.drop(columns=[\"SalePrice\"])\n",
    "\n",
    "##Remove ID\n",
    "train_id = X[[\"Id\"]]\n",
    "test_id = test[\"Id\"]\n",
    "X=X.drop(columns=[\"Id\"])\n",
    "test_no_id = test.drop(columns=[\"Id\"])\n",
    "\n",
    "#treat clearly categorical  numeric codes as string\n",
    "X[\"MSSubClass\"]= X[\"MSSubClass\"].astype(str)\n",
    "test_no_id[\"MSSubClass\"]=test_no_id[\"MSSubClass\"].astype(str)\n",
    "\n",
    "##Quick Peek\n",
    "display(train.head(3))\n",
    "print(\"\\n Top missing (train)\")\n",
    "print(X.isna().sum().sort_values(ascending=False).head(20))\n",
    "\n",
    "print(\"\\nTop missing (test):\")\n",
    "##print(test_no_id.isna()sum().sort_values(ascending=False).head(20))\n",
    "\n",
    "#quick target check\n",
    "print(\"\\nSalePrice(original)sumamry:\")\n",
    "print(train[\"SalePrice\"].describe())\n",
    "print(\"\\nSalePrice (log1p) mean/std:\", y.mean().round(4), y.std().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Preprocessiong Step\n",
    "##Combine test and train for processing\n",
    "n_train =X.shape[0]\n",
    "all_data = pd.concat([X, test_no_id], axis=0).reset_index(drop=True)\n",
    "print(\"Combined shape:\", all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Missing value imputation\n",
    "#Copy combined code\n",
    "all_data_imp = all_data.copy()\n",
    "\n",
    "## 1. Fill categorical \"no feature\" with \"None\"\n",
    "none_cols = [\"Alley\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"FireplaceQu\",\n",
    "             \"GarageType\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\"PoolQC\",\"Fence\",\"MiscFeature\"]\n",
    "for col in none_cols:\n",
    "    all_data_imp[col]=all_data_imp[col].fillna(\"None\")\n",
    "\n",
    "## 2. Fill numerical \"no feature\" with 0\n",
    "zero_cols =[\"BsmtFinSF1\",\"BsmtFinSF2\",\"BsmtUnfSF\",\"TotalBsmtSF\",\"BsmtFullBath\",\"BsmtHalfBath\",\"GarageYrBlt\",\"GarageCars\",\"GarageArea\"]\n",
    "for col in zero_cols:\n",
    "    all_data_imp[col]= all_data_imp[col].fillna(0)\n",
    "\n",
    "\n",
    "## 3. LotFrontage by Neighbhorhood median\n",
    "all_data_imp[\"LotFrontage\"]=all_data_imp.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.fillna(x.median())))\n",
    "\n",
    "## 4. Remaining categorical: fill with mode\n",
    "cat_cols = all_data_imp.select_dtypes(include=\"object\").columns\n",
    "for col in cat_cols:\n",
    "    if all_data_imp[col].isna().sum()>0:\n",
    "        all_data_imp[col]=all_data_imp[col].fillna(all_data_imp[col].mode()[0])\n",
    "        \n",
    "## 5. Remaining numeric:fill with median\n",
    "num_cols = all_data_imp.select_dtypes(include=\"object\").columns\n",
    "for col in num_cols:\n",
    "    if all_data_imp[col].isna().sum()>0:\n",
    "        all_data_imp[col]=all_data_imp[col].fillna(all_data_imp[col].median())\n",
    "\n",
    "\n",
    "all_data_imp[\"MasVnrArea\"]=all_data_imp[\"MasVnrArea\"].fillna(0)\n",
    "\n",
    "print(\" Missing values after imputation:\", all_data_imp.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code to check if any missing values after imputation\n",
    "\n",
    "missing_cols = all_data_imp.isna().sum()\n",
    "missing_cols = missing_cols[missing_cols>0].sort_values(ascending=False)\n",
    "\n",
    "print(missing_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Additional Imputation\n",
    "g= all_data_imp\n",
    "g[\"GarageYrBlt\"]=np.where(g[\"GarageType\"] == \"None\",0, g[\"GarageYrBlt\"].fillna(g[\"YearBuilt\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Missingness indicators for high NA\n",
    "hi_na_cols = [\"Alley\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType2\",\"FireplaceQu\",\"GarageType\",\n",
    "              \"GarageFinish\",\"GarageQual\",\"GarageCond\",\"PoolQC\",\"Fence\",\"MiscFeature\",\"MasVnrArea\",\"LotFrontage\"]\n",
    "\n",
    "for c in hi_na_cols:\n",
    "    all_data_imp[c + \"_was_missing\"]= all_data[c].isna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define Mapping for ordinal feature\n",
    "##Quality Mappings\n",
    "qual_mappings ={\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\":1, \"None\":0}\n",
    "\n",
    "Ordinal_cols ={\n",
    "    \"ExterQual\": qual_mappings,\n",
    "    \"ExterCond\": qual_mappings,\n",
    "    \"BsmtQual\": qual_mappings,\n",
    "    \"BsmtCond\": qual_mappings,\n",
    "    \"HeatingQC\": qual_mappings,\n",
    "    \"KitchenQual\": qual_mappings,\n",
    "    \"FireplaceQu\": qual_mappings,\n",
    "    \"GarageQual\": qual_mappings,\n",
    "    \"GarageCond\": qual_mappings,\n",
    "    \"PoolQC\": qual_mappings\n",
    "}\n",
    "\n",
    "all_data_enc = all_data_imp.copy()\n",
    "for col, mapping in Ordinal_cols.items():\n",
    "    all_data_enc[col]=all_data_enc[col].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##One hot encoding other categorical cols\n",
    "all_data_enc = pd.get_dummies(all_data_enc, drop_first=True)\n",
    "print(\"shape after encoding:\", all_data_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_enc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Detecting Skewness in numeric features\n",
    "from scipy.stats import skew\n",
    "##Seperate numeric features (after encoding, some 'ordinal mapped' are also numeric now)\n",
    "numeric_feats = [col for col in all_data_enc.columns\n",
    "                                                if\n",
    "                all_data_enc[col].dtypes != \"uint8\" and\n",
    "                all_data_enc[col].nunique()>10\n",
    "                ] \n",
    "all_data_enc.dtypes[(all_data_enc.dtypes != \"uint8\")].index  ##excludes one hot cols\n",
    "##Calculate skewness\n",
    "skewed_feats = all_data_enc[numeric_feats].apply(lambda x:skew(x.dropna())).sort_values(ascending = False)\n",
    "print(\"Top 10 skewed features:\\n\", skewed_feats.head(10))\n",
    "\n",
    "##Pick features with skew >.75\n",
    "skewed_cols = skewed_feats[abs(skewed_feats)> .75].index\n",
    "\n",
    "##Apply log1p\n",
    "all_data_enc[skewed_cols] = np.log1p(all_data_enc[skewed_cols])\n",
    "\n",
    "print(f\"Transformed {len(skewed_cols)} skewed numeric features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"PoolArea\", \"MiscVal\", \"3SsnPorch\",\"LowQualFinSF\"]\n",
    "all_data_final = all_data_enc.drop(columns = drop_cols)\n",
    "print(\"Final dataset shape:\", all_data_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_feat = all_data_final.copy()\n",
    "\n",
    "# 1. Total areas\n",
    "all_data_feat[\"TotalSF\"] = all_data_feat[\"TotalBsmtSF\"] + all_data_feat[\"1stFlrSF\"] + all_data_feat[\"2ndFlrSF\"]\n",
    "all_data_feat[\"TotalPorchSF\"] = (all_data_feat[\"OpenPorchSF\"] + \n",
    "                                 all_data_feat[\"EnclosedPorch\"] + all_data_feat[\"ScreenPorch\"] + \n",
    "                                 all_data_feat[\"WoodDeckSF\"])\n",
    "\n",
    "# 2. Bathrooms & rooms\n",
    "all_data_feat[\"TotalBath\"] = (all_data_feat[\"FullBath\"] + 0.5*all_data_feat[\"HalfBath\"] +\n",
    "                              all_data_feat[\"BsmtFullBath\"] + 0.5*all_data_feat[\"BsmtHalfBath\"])\n",
    "all_data_feat[\"TotalRooms\"] = all_data_feat[\"TotRmsAbvGrd\"] + all_data_feat[\"FullBath\"] + all_data_feat[\"HalfBath\"]\n",
    "\n",
    "# 3. Age features\n",
    "all_data_feat[\"HouseAge\"] = all_data_feat[\"YrSold\"] - all_data_feat[\"YearBuilt\"]\n",
    "all_data_feat[\"RemodAge\"] = all_data_feat[\"YrSold\"] - all_data_feat[\"YearRemodAdd\"]\n",
    "all_data_feat[\"GarageAge\"] = np.where(all_data_feat[\"GarageYrBlt\"] > 0,\n",
    "                                      all_data_feat[\"YrSold\"] - all_data_feat[\"GarageYrBlt\"], 0)\n",
    "\n",
    "# 4. Quality × size\n",
    "all_data_feat[\"OverallQual_GrLivArea\"] = all_data_feat[\"OverallQual\"] * all_data_feat[\"GrLivArea\"]\n",
    "all_data_feat[\"OverallQual_TotSF\"] = all_data_feat[\"OverallQual\"] * all_data_feat[\"TotalSF\"]\n",
    "\n",
    "# 5. Ratios\n",
    "all_data_feat[\"RoomsPerArea\"] = all_data_feat[\"TotRmsAbvGrd\"] / (all_data_feat[\"GrLivArea\"] + 1e-5)\n",
    "all_data_feat[\"LotAreaPerRoom\"] = all_data_feat[\"LotArea\"] / (all_data_feat[\"TotRmsAbvGrd\"] + 1)\n",
    "\n",
    "print(\"✅ Final dataset with engineered features:\", all_data_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_more = [\n",
    "    \"Utilities\", \"Street\", \"Condition2\", \"RoofMatl\", \"Heating\", \n",
    "    \"GarageArea\",   # keep GarageCars\n",
    "    \"TotRmsAbvGrd\"  # we keep TotalRooms instead\n",
    "]\n",
    "\n",
    "all_data_clean = all_data_feat.drop(columns=[c for c in drop_more if c in all_data_feat.columns])\n",
    "\n",
    "print(\"✅ Final dataset after feature selection:\", all_data_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Save number of training rows\n",
    "n_train = X.shape[0]\n",
    "\n",
    "# Split back into train/test from combined processed data\n",
    "train_processed = all_data_clean.iloc[:n_train, :].copy()\n",
    "test_processed = all_data_clean.iloc[n_train:, :].copy()\n",
    "\n",
    "# Scale features for linear models\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_processed)\n",
    "test_scaled = scaler.transform(test_processed)\n",
    "\n",
    "print(\"Train scaled shape:\", train_scaled.shape)\n",
    "print(\"Test scaled shape:\", test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the feature\n",
    "feature = \"GrLivArea\"\n",
    "\n",
    "# Original values\n",
    "train_raw = train_processed[feature].values[:5]   # first 5 rows\n",
    "test_raw  = test_processed[feature].values[:5]\n",
    "\n",
    "# Scaled values (already fit on train)\n",
    "train_scaled_vals = train_scaled[:5, list(train_processed.columns).index(feature)]\n",
    "test_scaled_vals  = test_scaled[:5, list(train_processed.columns).index(feature)]\n",
    "\n",
    "print(f\"Feature: {feature}\\n\")\n",
    "\n",
    "print(\"Train raw values:   \", train_raw)\n",
    "print(\"Train scaled values:\", np.round(train_scaled_vals, 3))\n",
    "\n",
    "print(\"\\nTest raw values:    \", test_raw)\n",
    "print(\"Test scaled values: \", np.round(test_scaled_vals, 3))\n",
    "\n",
    "print(\"\\nScaler parameters → mean =\", round(scaler.mean_[list(train_processed.columns).index(feature)], 2),\n",
    "      \"std =\", round(scaler.scale_[list(train_processed.columns).index(feature)], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Target variable (log-transform helps normality of SalePrice)\n",
    "y = np.log1p(train[\"SalePrice\"])   # train[\"SalePrice\"] comes from original training dataset\n",
    "\n",
    "# Candidate alpha values to search\n",
    "alphas = [0.1, 1, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "\n",
    "# RidgeCV does cross-validation internally\n",
    "ridge = RidgeCV(alphas=alphas, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "\n",
    "ridge.fit(train_scaled, y)\n",
    "\n",
    "# Best alpha\n",
    "print(\"Best alpha:\", ridge.alpha_)\n",
    "\n",
    "# CV RMSE (converted from neg MSE)\n",
    "cv_rmse = np.sqrt(-ridge.best_score_)\n",
    "print(\"CV RMSE:\", cv_rmse)\n",
    "\n",
    "# Predict on test set (already scaled)\n",
    "ridge_preds = np.expm1(ridge.predict(test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Get out-of-fold predictions using cross-validation\n",
    "train_pred_log = cross_val_predict(ridge, train_scaled, y, cv=5)\n",
    "\n",
    "# Convert back to original SalePrice scale\n",
    "train_pred = np.expm1(train_pred_log)\n",
    "y_true = np.expm1(y)\n",
    "\n",
    "# RMSE in original scale\n",
    "rmse = np.sqrt(mean_squared_error(y_true, train_pred))\n",
    "print(\"Training CV RMSE (original scale):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(y_true, train_pred, alpha=0.3)\n",
    "plt.xlabel(\"Actual SalePrice\")\n",
    "plt.ylabel(\"Predicted SalePrice\")\n",
    "plt.title(\"Ridge Regression: Actual vs Predicted\")\n",
    "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Target (log-transformed SalePrice)\n",
    "y = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "# ----- Ridge -----\n",
    "alphas = [0.1, 1, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "ridge = RidgeCV(alphas=alphas, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "ridge.fit(train_scaled, y)\n",
    "ridge_rmse = np.sqrt(-ridge.best_score_)\n",
    "\n",
    "# ----- Lasso -----\n",
    "lasso = LassoCV(alphas=np.logspace(-4, -0.5, 50), cv=5, max_iter=10000, random_state=42)\n",
    "lasso.fit(train_scaled, y)\n",
    "lasso_rmse = np.sqrt(np.mean(-cross_val_score(lasso, train_scaled, y, cv=5, \n",
    "                                              scoring=\"neg_mean_squared_error\")))\n",
    "\n",
    "# ----- ElasticNet -----\n",
    "elasticnet = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], \n",
    "                          alphas=np.logspace(-4, -0.5, 50), \n",
    "                          cv=5, max_iter=10000, random_state=42)\n",
    "elasticnet.fit(train_scaled, y)\n",
    "elastic_rmse = np.sqrt(np.mean(-cross_val_score(elasticnet, train_scaled, y, cv=5, \n",
    "                                                scoring=\"neg_mean_squared_error\")))\n",
    "\n",
    "# ----- Compare -----\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Ridge\", \"Lasso\", \"ElasticNet\"],\n",
    "    \"Best Alpha\": [ridge.alpha_, lasso.alpha_, elasticnet.alpha_],\n",
    "    \"CV RMSE (log)\": [ridge_rmse, lasso_rmse, elastic_rmse]\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# ----- Lasso -----\n",
    "lasso = LassoCV(alphas=np.logspace(-4, -0.5, 50),\n",
    "                cv=5, max_iter=50000, random_state=42)\n",
    "lasso.fit(train_scaled, y)\n",
    "lasso_rmse = np.sqrt(np.mean(-cross_val_score(lasso, train_scaled, y, \n",
    "                                              cv=5, scoring=\"neg_mean_squared_error\")))\n",
    "print(\"Best alpha (Lasso):\", lasso.alpha_)\n",
    "print(\"CV RMSE (Lasso):\", lasso_rmse)\n",
    "\n",
    "# ----- ElasticNet -----\n",
    "elasticnet = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1],\n",
    "                          alphas=np.logspace(-4, -0.5, 50),\n",
    "                          cv=5, max_iter=50000, random_state=42)\n",
    "elasticnet.fit(train_scaled, y)\n",
    "elastic_rmse = np.sqrt(np.mean(-cross_val_score(elasticnet, train_scaled, y, \n",
    "                                                cv=5, scoring=\"neg_mean_squared_error\")))\n",
    "print(\"Best alpha (ElasticNet):\", elasticnet.alpha_)\n",
    "print(\"Best l1_ratio (ElasticNet):\", elasticnet.l1_ratio_)\n",
    "print(\"CV RMSE (ElasticNet):\", elastic_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# If lasso is already fit on train_scaled and y (log1p SalePrice):\n",
    "lasso_preds_test = np.expm1(lasso.predict(test_scaled))\n",
    "\n",
    "# Build submission\n",
    "sub = pd.DataFrame({\n",
    "    \"Id\": test[\"Id\"],          \n",
    "    \"SalePrice\": lasso_preds_test\n",
    "})\n",
    "\n",
    "sub_path = \"submission_lasso.csv\"\n",
    "sub.to_csv(sub_path, index=False)\n",
    "print(\"Saved:\", sub_path, \"→\", sub.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c house-prices-advanced-regression-techniques -f submission_lasso.csv -m \"LassoCV baseline submission\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Features and target\n",
    "X_tr = train_processed\n",
    "X_te = test_processed\n",
    "y_log = np.log1p(train[\"SalePrice\"])   # log1p for Kaggle scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Features and target\n",
    "X_tr = train_processed\n",
    "X_te = test_processed\n",
    "y_log = np.log1p(train[\"SalePrice\"])\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_rmse = []\n",
    "\n",
    "for tr_idx, val_idx in kf.split(X_tr):\n",
    "    Xtr, Xval = X_tr.iloc[tr_idx], X_tr.iloc[val_idx]\n",
    "    ytr, yval = y_log.iloc[tr_idx], y_log.iloc[val_idx]\n",
    "\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=2000,       # fixed number of trees\n",
    "        learning_rate=0.01,\n",
    "        max_depth=4,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\"       # or \"auto\" if hist not supported\n",
    "    )\n",
    "\n",
    "    xgb.fit(Xtr, ytr, verbose=False)\n",
    "\n",
    "    pred_val = xgb.predict(Xval)\n",
    "    rmse = np.sqrt(mean_squared_error(yval, pred_val))\n",
    "    cv_rmse.append(rmse)\n",
    "\n",
    "print(\"XGB 5-fold CV RMSE (log):\", np.mean(cv_rmse), \"+/-\", np.std(cv_rmse))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_final = XGBRegressor(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=4,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"\n",
    ")\n",
    "\n",
    "xgb_final.fit(X_tr, y_log)\n",
    "xgb_preds = np.expm1(xgb_final.predict(X_te))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Blent Both Lasso and xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blend_preds = 0.5 * lasso_preds_test + 0.5 * xgb_preds\n",
    "\n",
    "sub_blend = pd.DataFrame({\n",
    "    \"Id\": test[\"Id\"],\n",
    "    \"SalePrice\": blend_preds\n",
    "})\n",
    "\n",
    "sub_blend.to_csv(\"submission_blend.csv\", index=False)\n",
    "print(\"✅ Saved submission_blend.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c house-prices-advanced-regression-techniques -f submission_blend.csv -m \"LassoCV xgboostCV blend submission\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Finding best blend weight of xgboost and lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "lasso_oof = np.zeros(len(X_tr))\n",
    "xgb_oof = np.zeros(len(X_tr))\n",
    "\n",
    "for tr_idx, val_idx in kf.split(X_tr):\n",
    "    Xtr, Xval = X_tr.iloc[tr_idx], X_tr.iloc[val_idx]\n",
    "    ytr, yval = y_log.iloc[tr_idx], y_log.iloc[val_idx]\n",
    "\n",
    "    # ---- Scale for Lasso ----\n",
    "    scaler = StandardScaler()\n",
    "    Xtr_scaled = scaler.fit_transform(Xtr)\n",
    "    Xval_scaled = scaler.transform(Xval)\n",
    "\n",
    "    # ---- Lasso ----\n",
    "    lasso.fit(Xtr_scaled, ytr)\n",
    "    lasso_oof[val_idx] = lasso.predict(Xval_scaled)\n",
    "\n",
    "    # ---- XGB ----\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=4,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\"\n",
    "    )\n",
    "    xgb.fit(Xtr, ytr, verbose=False)\n",
    "    xgb_oof[val_idx] = xgb.predict(Xval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rmse = float(\"inf\")\n",
    "best_w = 0\n",
    "\n",
    "for w in np.linspace(0, 1, 21):   # 0.0, 0.05, 0.10, ..., 1.0\n",
    "    blend = w * lasso_oof + (1 - w) * xgb_oof\n",
    "    rmse = np.sqrt(mean_squared_error(y_log, blend))\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_w = w\n",
    "\n",
    "print(\"Best weight for Lasso:\", round(best_w, 2))\n",
    "print(\"CV RMSE with blend:\", best_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [0.30,0.32,0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65]\n",
    "for w in candidates:\n",
    "    preds = w*lasso_preds_test + (1-w)*xgb_preds\n",
    "    pd.DataFrame({\"Id\": test[\"Id\"], \"SalePrice\": preds})\\\n",
    "      .to_csv(f\"sub_blend_{w:.2f}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c house-prices-advanced-regression-techniques -f sub_blend_0.35.csv -m \"LassoCV xgboostCV blend(.35) submission\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Stacking with a meta-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Stack training (use OOF preds of lasso & xgb)\n",
    "stack_train = np.c_[lasso_oof, xgb_oof]\n",
    "stack_test  = np.c_[np.log1p(lasso_preds_test), np.log1p(xgb_preds)]\n",
    "\n",
    "meta = Ridge(alpha=1.0, random_state=42)\n",
    "meta.fit(stack_train, y_log)\n",
    "\n",
    "stack_preds_log = meta.predict(stack_test)\n",
    "stack_preds = np.expm1(stack_preds_log)\n",
    "\n",
    "pd.DataFrame({\"Id\": test[\"Id\"], \"SalePrice\": stack_preds}).to_csv(\"submission_stack.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c house-prices-advanced-regression-techniques -f submission_stack.csv -m \"LassoCV xgboostCV Stack submission\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-model stacking pipeline with Lasso + XGB + LightGBM stacked using a Ridge meta-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Base models\n",
    "lasso = LassoCV(alphas=np.logspace(-4, -0.5, 50), max_iter=50000, random_state=42)\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=4,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"\n",
    ")\n",
    "lgbm = LGBMRegressor(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=4,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# OOF + Test predictions\n",
    "oof_lasso, oof_xgb, oof_lgbm = np.zeros(len(train_processed)), np.zeros(len(train_processed)), np.zeros(len(train_processed))\n",
    "test_lasso, test_xgb, test_lgbm = np.zeros(len(test_processed)), np.zeros(len(test_processed)), np.zeros(len(test_processed))\n",
    "\n",
    "cv_rmse_lasso, cv_rmse_xgb, cv_rmse_lgbm = [], [], []\n",
    "\n",
    "y_log = np.log1p(train[\"SalePrice\"])\n",
    "X, T = train_processed, test_processed\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = y_log.iloc[tr_idx], y_log.iloc[val_idx]\n",
    "\n",
    "    # ---- Lasso ----\n",
    "    lasso.fit(X_tr, y_tr)\n",
    "    pred_val = lasso.predict(X_val)\n",
    "    oof_lasso[val_idx] = pred_val\n",
    "    test_lasso += lasso.predict(T) / kf.n_splits\n",
    "    cv_rmse_lasso.append(np.sqrt(mean_squared_error(y_val, pred_val)))\n",
    "\n",
    "    # ---- XGB ----\n",
    "    xgb.fit(X_tr, y_tr, verbose=False)\n",
    "    pred_val = xgb.predict(X_val)\n",
    "    oof_xgb[val_idx] = pred_val\n",
    "    test_xgb += xgb.predict(T) / kf.n_splits\n",
    "    cv_rmse_xgb.append(np.sqrt(mean_squared_error(y_val, pred_val)))\n",
    "\n",
    "    # ---- LGBM ----\n",
    "    lgbm.fit(X_tr, y_tr)\n",
    "    pred_val = lgbm.predict(X_val)\n",
    "    oof_lgbm[val_idx] = pred_val\n",
    "    test_lgbm += lgbm.predict(T) / kf.n_splits\n",
    "    cv_rmse_lgbm.append(np.sqrt(mean_squared_error(y_val, pred_val)))\n",
    "\n",
    "print(\"CV RMSE Lasso:\", np.mean(cv_rmse_lasso), \"+/-\", np.std(cv_rmse_lasso))\n",
    "print(\"CV RMSE XGB:\", np.mean(cv_rmse_xgb), \"+/-\", np.std(cv_rmse_xgb))\n",
    "print(\"CV RMSE LGBM:\", np.mean(cv_rmse_lgbm), \"+/-\", np.std(cv_rmse_lgbm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack OOF predictions\n",
    "X_meta = np.vstack([oof_lasso, oof_xgb, oof_lgbm]).T\n",
    "T_meta = np.vstack([test_lasso, test_xgb, test_lgbm]).T\n",
    "\n",
    "ridge_meta = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5)\n",
    "ridge_meta.fit(X_meta, y_log)\n",
    "\n",
    "# OOF meta-preds for CV RMSE\n",
    "oof_meta = ridge_meta.predict(X_meta)\n",
    "cv_rmse_meta = np.sqrt(mean_squared_error(y_log, oof_meta))\n",
    "\n",
    "print(\"✅ CV RMSE Stacked Model:\", cv_rmse_meta)\n",
    "\n",
    "# Final test predictions\n",
    "final_preds = np.expm1(ridge_meta.predict(T_meta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"Id\": test[\"Id\"],\n",
    "    \"SalePrice\": final_preds\n",
    "})\n",
    "submission.to_csv(\"submission_3Model_stack.csv\", index=False)\n",
    "print(\"Submission file created: submission_3Model_stack.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c house-prices-advanced-regression-techniques -f submission_3Model_stack.csv -m \"LassoCV xgboostCV lightgbm_Stack submission\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
