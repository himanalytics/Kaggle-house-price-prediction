{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab99e48",
   "metadata": {},
   "source": [
    "🏡 Kaggle Notebook: House Price Prediction with Lasso, XGBoost & Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd97b5",
   "metadata": {},
   "source": [
    "# 🏡 House Prices: Advanced Regression Techniques\n",
    "\n",
    "This notebook walks through a **complete end-to-end pipeline** for predicting house prices.  \n",
    "We cover:\n",
    "\n",
    "1. Data Loading & Exploration  \n",
    "2. Handling Missing Values  \n",
    "3. Feature Engineering  \n",
    "4. Encoding & Scaling  \n",
    "5. Model Training: **LassoCV** & **XGBoost**  \n",
    "6. Blending Models  \n",
    "7. **Stacking with Ridge meta-model (Best Score: ~0.11845 RMSE)** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c899e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 1. Imports & setup\n",
    "# ================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, Lasso, Ridge\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "N_SPLITS = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10248413",
   "metadata": {},
   "source": [
    "## 📂 Step 2: Load Data\n",
    "We load the **train** and **test** datasets provided by Kaggle.  \n",
    "Target variable: `SalePrice` (we will log-transform it for stability).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7edaed03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1460, 81)  Test: (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(r\"C:\\Users\\chand\\Desktop\\ML Projects\\Kaggle- house-price\")\n",
    "train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "test  = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "\n",
    "print(\"Train:\", train.shape, \" Test:\", test.shape)\n",
    "\n",
    "# Target on log scale for stable training\n",
    "y = np.log1p(train[\"SalePrice\"])\n",
    "train_id = train[\"Id\"].copy()\n",
    "test_id  = test[\"Id\"].copy()\n",
    "\n",
    "# Work on copies; drop identifiers/target from features\n",
    "X_tr_raw = train.drop(columns=[\"Id\", \"SalePrice\"]).copy()\n",
    "X_te_raw = test.drop(columns=[\"Id\"]).copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1de05e",
   "metadata": {},
   "source": [
    "## 3) Preprocessing (safe leak-free wrt target)\n",
    "We **concatenate train+test** to ensure consistent encoding & imputation across both splits (this is allowed in Kaggle since test features are known and the target is not used).\n",
    "\n",
    "Strategies:\n",
    "- Fill domain “no feature” categoricals with `\"None\"`\n",
    "- Fill numerical **no-feature** columns with `0`\n",
    "- `LotFrontage` by neighborhood median\n",
    "- Remaining categoricals → mode; remaining numerics → median\n",
    "- Cast `MSSubClass` to string (categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b70c5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([X_tr_raw, X_te_raw], axis=0).reset_index(drop=True)\n",
    "\n",
    "# treat MSSubClass as categorical\n",
    "all_data[\"MSSubClass\"] = all_data[\"MSSubClass\"].astype(str)\n",
    "\n",
    "# 3.1 categorical None-fill\n",
    "none_cols = [\"Alley\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\n",
    "             \"FireplaceQu\",\"GarageType\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\n",
    "             \"PoolQC\",\"Fence\",\"MiscFeature\"]\n",
    "for c in none_cols:\n",
    "    if c in all_data.columns:\n",
    "        all_data[c] = all_data[c].fillna(\"None\")\n",
    "\n",
    "# 3.2 numerical zero-fill (absence)\n",
    "zero_cols = [\"BsmtFinSF1\",\"BsmtFinSF2\",\"BsmtUnfSF\",\"TotalBsmtSF\",\n",
    "             \"BsmtFullBath\",\"BsmtHalfBath\",\"GarageYrBlt\",\"GarageCars\",\"GarageArea\"]\n",
    "for c in zero_cols:\n",
    "    if c in all_data.columns:\n",
    "        all_data[c] = all_data[c].fillna(0)\n",
    "\n",
    "# 3.3 LotFrontage by neighborhood median\n",
    "if \"LotFrontage\" in all_data.columns and \"Neighborhood\" in all_data.columns:\n",
    "    all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"] \\\n",
    "                                      .transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# 3.4 remaining\n",
    "for c in all_data.select_dtypes(include=\"object\").columns:\n",
    "    all_data[c] = all_data[c].fillna(all_data[c].mode()[0])\n",
    "for c in all_data.select_dtypes(exclude=\"object\").columns:\n",
    "    all_data[c] = all_data[c].fillna(all_data[c].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d34a2b",
   "metadata": {},
   "source": [
    "## 4) Feature engineering\n",
    "Some additions for this dataset:\n",
    "- `TotalSF` (total livable area)\n",
    "- `TotalBath` (full + 0.5×half, incl. basement)\n",
    "- `OverallQual_GrLivArea` (quality × size).\n",
    "We also drop a few noisy/low-informative columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97587ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New features\n",
    "all_data[\"TotalSF\"] = all_data[\"TotalBsmtSF\"] + all_data[\"1stFlrSF\"] + all_data[\"2ndFlrSF\"]\n",
    "all_data[\"TotalBath\"] = (all_data[\"FullBath\"] + 0.5*all_data[\"HalfBath\"] +\n",
    "                         all_data[\"BsmtFullBath\"] + 0.5*all_data[\"BsmtHalfBath\"])\n",
    "all_data[\"OverallQual_GrLivArea\"] = all_data[\"OverallQual\"] * all_data[\"GrLivArea\"]\n",
    "\n",
    "# Drop a few consistently unhelpful columns\n",
    "drop_cols = [\"PoolArea\",\"MiscVal\",\"3SsnPorch\",\"LowQualFinSF\"]\n",
    "drop_more = [\"Utilities\",\"Street\",\"Condition2\",\"RoofMatl\",\"Heating\",\n",
    "             \"GarageArea\",           # keep GarageCars\n",
    "             \"TotRmsAbvGrd\"]         # keep TotalRooms proxy features\n",
    "for c in drop_cols + drop_more:\n",
    "    if c in all_data.columns:\n",
    "        all_data = all_data.drop(columns=c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54338a3",
   "metadata": {},
   "source": [
    "## 🔢 Step 5: Encoding\n",
    "We encode categorical features:\n",
    "- Ordinal encoding for quality ratings.  \n",
    "- One-hot encoding for all remaining categorical features.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7e888f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All-data shape after encoding: (2919, 218)\n"
     ]
    }
   ],
   "source": [
    "qual_map = {\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\":2, \"Po\":1, \"None\":0}\n",
    "ordinal_cols = [\"ExterQual\",\"ExterCond\",\"BsmtQual\",\"BsmtCond\",\"HeatingQC\",\n",
    "                \"KitchenQual\",\"FireplaceQu\",\"GarageQual\",\"GarageCond\",\"PoolQC\"]\n",
    "for c in ordinal_cols:\n",
    "    if c in all_data.columns:\n",
    "        all_data[c] = all_data[c].map(qual_map)\n",
    "\n",
    "all_data = pd.get_dummies(all_data, drop_first=True)\n",
    "\n",
    "print(\"All-data shape after encoding:\", all_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d164963",
   "metadata": {},
   "source": [
    "## 6) Fix skewness (numeric only)\n",
    "Log1p-transform skewed numeric features (|skew| > 0.75).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7f02b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = all_data.select_dtypes(include=[np.number]).columns\n",
    "skewed = all_data[numeric_cols].apply(lambda x: x.dropna().skew()).abs()\n",
    "skew_cols = skewed[skewed > 0.75].index\n",
    "all_data[skew_cols] = np.log1p(all_data[skew_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd196843",
   "metadata": {},
   "source": [
    "## 7) Split back into train/test and prepare scalers for linear models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d41cf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix: (1460, 218)  Test matrix: (1459, 218)\n"
     ]
    }
   ],
   "source": [
    "n_train = X_tr_raw.shape[0]\n",
    "X_train = all_data.iloc[:n_train, :].copy()\n",
    "X_test  = all_data.iloc[n_train:, :].copy()\n",
    "\n",
    "# We'll scale inside each CV fold for Lasso when creating OOF predictions\n",
    "print(\"Train matrix:\", X_train.shape, \" Test matrix:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded54a7f",
   "metadata": {},
   "source": [
    "## 8) Helper: OOF predictions (essential for correct stacking)\n",
    "This function returns:\n",
    "- `oof_log`: OOF predictions on **log scale** aligned to `y`\n",
    "- `test_log_mean`: mean test prediction across folds on **log scale**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67c13dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof_lasso(X, y, X_test, seed=SEED, n_splits=N_SPLITS):\n",
    "    # tune alpha once on a global (train-only) scale version to keep it fast/reliable\n",
    "    scaler_global = StandardScaler().fit(X)\n",
    "    X_scaled_global = scaler_global.transform(X)\n",
    "\n",
    "    lasso_cv = LassoCV(alphas=np.logspace(-4, -0.5, 50),\n",
    "                       cv=5, max_iter=50000, random_state=seed)\n",
    "    lasso_cv.fit(X_scaled_global, y)\n",
    "    best_alpha = lasso_cv.alpha_\n",
    "    print(f\"[Lasso] Best alpha: {best_alpha:.6f}\")\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros(len(X))\n",
    "    test_fold_pred = np.zeros((n_splits, X_test.shape[0]))\n",
    "\n",
    "    for i, (tr_idx, va_idx) in enumerate(kf.split(X)):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr = y.iloc[tr_idx]\n",
    "\n",
    "        # scale per-fold to avoid leakage\n",
    "        scaler = StandardScaler().fit(X_tr)\n",
    "        X_tr_sc = scaler.transform(X_tr)\n",
    "        X_va_sc = scaler.transform(X_va)\n",
    "        X_te_sc = scaler.transform(X_test)\n",
    "\n",
    "        model = Lasso(alpha=best_alpha, max_iter=50000, random_state=seed)\n",
    "        model.fit(X_tr_sc, y_tr)\n",
    "\n",
    "        oof[va_idx] = model.predict(X_va_sc)             # already in log scale\n",
    "        test_fold_pred[i, :] = model.predict(X_te_sc)    # log scale\n",
    "\n",
    "    oof_log = oof\n",
    "    test_log_mean = test_fold_pred.mean(axis=0)\n",
    "    rmse = np.sqrt(mean_squared_error(y, oof_log))\n",
    "    print(f\"[Lasso] OOF RMSE (log): {rmse:.5f}\")\n",
    "    return oof_log, test_log_mean, best_alpha\n",
    "\n",
    "\n",
    "def get_oof_xgb(X, y, X_test, seed=SEED, n_splits=N_SPLITS):\n",
    "    params = dict(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=4,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=seed,\n",
    "        tree_method=\"hist\"\n",
    "    )\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros(len(X))\n",
    "    test_fold_pred = np.zeros((n_splits, X_test.shape[0]))\n",
    "\n",
    "    for i, (tr_idx, va_idx) in enumerate(kf.split(X)):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr = y.iloc[tr_idx]\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(X_tr, y_tr, verbose=False)\n",
    "\n",
    "        oof[va_idx] = model.predict(X_va)          # log scale\n",
    "        test_fold_pred[i, :] = model.predict(X_test)\n",
    "\n",
    "    oof_log = oof\n",
    "    test_log_mean = test_fold_pred.mean(axis=0)\n",
    "    rmse = np.sqrt(mean_squared_error(y, oof_log))\n",
    "    print(f\"[XGB ] OOF RMSE (log): {rmse:.5f}\")\n",
    "    return oof_log, test_log_mean, params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21780f31",
   "metadata": {},
   "source": [
    "## 9) Train base models & create submissions\n",
    "- `submission_blend.csv` — a simple 0.35/0.65 blend (good baseline)\n",
    "- `submission_stack.csv` — Ridge stacker on OOFs (your best previously)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e16dad47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lasso] Best alpha: 0.003728\n",
      "[Lasso] OOF RMSE (log): 0.13161\n",
      "[XGB ] OOF RMSE (log): 0.12805\n",
      "Saved: submission_blend2.csv\n"
     ]
    }
   ],
   "source": [
    "# Base OOFs (LOG scale)\n",
    "lasso_oof_log, lasso_test_log, best_alpha = get_oof_lasso(X_train, y, X_test)\n",
    "xgb_oof_log,   xgb_test_log,   xgb_params   = get_oof_xgb(X_train, y, X_test)\n",
    "\n",
    "# Blend on ORIGINAL scale for a baseline submission\n",
    "lasso_test_pred = np.expm1(lasso_test_log)\n",
    "xgb_test_pred   = np.expm1(xgb_test_log)\n",
    "\n",
    "BLEND_W = 0.35  # (found good in prior experiments)\n",
    "blend_pred = BLEND_W * lasso_test_pred + (1 - BLEND_W) * xgb_test_pred\n",
    "\n",
    "pd.DataFrame({\"Id\": test_id, \"SalePrice\": blend_pred}).to_csv(\"submission_blend2.csv\", index=False)\n",
    "print(\"Saved: submission_blend2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1251cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to House Prices - Advanced Regression Techniques\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/35.1k [00:00<?, ?B/s]\n",
      " 46%|████▌     | 16.0k/35.1k [00:00<00:00, 90.9kB/s]\n",
      "100%|██████████| 35.1k/35.1k [00:01<00:00, 30.7kB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c house-prices-advanced-regression-techniques -f submission_blend2.csv -m \"LassoCV xgboostCV blend submission\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50788779",
   "metadata": {},
   "source": [
    "## 10) Stacking (Ridge meta-model)\n",
    "- Train meta on **OOF (log)** predictions  \n",
    "- Predict test on **mean fold test preds (log)**  \n",
    "- Convert final to **original scale** for submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f486f47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stack] OOF RMSE (log): 0.12534\n",
      "Saved: submission_stack3.csv\n"
     ]
    }
   ],
   "source": [
    "# Stack training matrix (log scale features)\n",
    "stack_train = np.vstack([lasso_oof_log, xgb_oof_log]).T\n",
    "stack_test  = np.vstack([lasso_test_log, xgb_test_log]).T\n",
    "\n",
    "meta = Ridge(alpha=1.0, random_state=SEED)\n",
    "meta.fit(stack_train, y)\n",
    "\n",
    "# CV estimate on stacker (optional quick sanity check)\n",
    "stack_pred_oof = meta.predict(stack_train)\n",
    "stack_rmse_log = np.sqrt(mean_squared_error(y, stack_pred_oof))\n",
    "print(f\"[Stack] OOF RMSE (log): {stack_rmse_log:.5f}\")\n",
    "\n",
    "# Test predictions\n",
    "stack_pred_log = meta.predict(stack_test)\n",
    "stack_pred = np.expm1(stack_pred_log)\n",
    "\n",
    "pd.DataFrame({\"Id\": test_id, \"SalePrice\": stack_pred}).to_csv(\"submission_stack3.csv\", index=False)\n",
    "print(\"Saved: submission_stack3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39661865",
   "metadata": {},
   "source": [
    "# ✅ Conclusion\n",
    "- We built a **robust pipeline** for house price prediction.  \n",
    "- Preprocessing: handled missing values, feature engineering, encoding, scaling.  \n",
    "- Models: LassoCV, XGBoost, Blending, and **Stacking**.  \n",
    "- Best performance came from **Stacking with Ridge meta-model** (~0.1184 RMSE).  \n",
    "\n",
    "🚀 Next steps:  \n",
    "- Try **LightGBM** / **CatBoost**.  \n",
    "- More **feature interactions**.  \n",
    "- Hyperparameter tuning with **Optuna**.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
